<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://ece396-source-separation.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ece396-source-separation.github.io/" rel="alternate" type="text/html" /><updated>2021-03-23T07:35:36-04:00</updated><id>https://ece396-source-separation.github.io/feed.xml</id><title type="html">Source Separation</title><subtitle>An amazing website.</subtitle><author><name>Richard Lee and Matt Kribs</name><email>Lee66@cooper.edu</email></author><entry><title type="html">Mid-Semester Project Updates</title><link href="https://ece396-source-separation.github.io/project-updates/" rel="alternate" type="text/html" title="Mid-Semester Project Updates" /><published>2021-03-20T00:00:00-04:00</published><updated>1969-12-31T06:59:59-05:00</updated><id>https://ece396-source-separation.github.io/project-updates</id><content type="html" xml:base="https://ece396-source-separation.github.io/project-updates/">&lt;p&gt;Imagine walking into a crowded, noisy restaurant -&lt;/p&gt;

&lt;p&gt;Well, these are still quarantine times, so this type of scenario is less common, but presumably, when life returns to normal such a scenario could occur.&lt;/p&gt;

&lt;p&gt;You sit across the table from your friend. It’s noisy, with many other conversations going on around you, making it rather difficult to hear what she is saying.&lt;/p&gt;

&lt;h1 id=&quot;cocktail-party-problem&quot;&gt;Cocktail Party Problem&lt;/h1&gt;

&lt;p&gt;The Cocktail Party Effect is the, “ability for people to focus their auditory attention on one source,” whether that be a friend at a party, or a waiter in a restaurant. It has been said that listeners are able to segregate the different audio sources, and tune into one, as opposed to the others&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;However, the ability to “tune in” to a single voice is highly dependent on a number of features, including speaker pitch, location, rate of speech, and the listener’s hearing capability. In other words: different people, when presented with a situation with multiple speakers, will only be able to pick out a selection of the words that are being spoken, and not necessarily only from the speaker of interest.&lt;/p&gt;

&lt;p&gt;Moreover, if a person only has one functional ear, or is hard of hearing, the task becomes even more difficult: with only one ear, it is difficult to determine locality of the speaker, and when hard of hearing, all sounds come through with limited fidelity.&lt;/p&gt;

&lt;p&gt;So what if there were ways that we could make a device that could ‘tune out’ conflicting voices, listening only to the speaker of interest? Such a device would not only have to be able separate voices from a mixture, but also do it in a semi-real time fashion in order for it to be useful in a restaurant setting. Let’s look more at ways to accomplish that first requirement.&lt;/p&gt;

&lt;h1 id=&quot;separating-voice-mixtures&quot;&gt;Separating Voice Mixtures&lt;/h1&gt;

&lt;h2 id=&quot;blind-source-separation&quot;&gt;Blind Source Separation&lt;/h2&gt;

&lt;p&gt;The traditional way to separate voices from a mixture is Blind Source Separation (BSS). “Blind” refers to the fact that the process by which the voices were mixed is unknown. BSS algorithms assume properties of the signal sources and the mixing processes and use those assumptions to try to reconstruct the original audio.&lt;/p&gt;

&lt;p&gt;One such algorithm is &lt;strong&gt;Independent Component Analysis&lt;/strong&gt; which requires that there are at least as many microphones as there are voices in the mixture, and relies on the assumption that the signals are non-Gaussian and independent, which are not necessarily always true.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Insert audio samples here&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition, the necessity for multiple microphones makes this algorithm difficult to deploy in practice.&lt;/p&gt;

&lt;h2 id=&quot;neural-networks-for-audio-separation&quot;&gt;Neural Networks for Audio Separation&lt;/h2&gt;

&lt;p&gt;Artificial Neural Networks, or also referred to as neural networks, have proven to be very useful in a wide variety of tasks, including source separation. Neural networks, using large amounts of training data, can capture complex relationships that can be used for inference. In the case of source separation, a neural network can characterize how much of each audio slice belongs to each speaker.&lt;/p&gt;

&lt;p&gt;Neural networks are not limited in the same way that BSS methods like ICA are - so long as the training data are representative of the testing data, there are fewer limitations on the properties of the original sources or the mixture.&lt;/p&gt;

&lt;h3 id=&quot;aside-spectrograms&quot;&gt;Aside: Spectrograms&lt;/h3&gt;

&lt;p&gt;A commonly used tool in the field of audio processing is the &lt;strong&gt;spectrogram&lt;/strong&gt;, which is a 2D representation of an audio signal, generated using a Short Time Fourier Transform (STFT) with frequencies on one axis and time on another. The intensity of each ‘pixel’ represents the intensity of a frequency at any given time. Conventional wisdom was always that spectrograms are &lt;em&gt;vital&lt;/em&gt; tools for source separation, as intuitively, separating the frequencies should assist with the separation.&lt;/p&gt;

&lt;p&gt;However, in recent literature&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, it was found that neural networks could achieve very accurate results without performing the time-consuming STFT operation and its inverse.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Insert audio samples here&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;real-time-considerations&quot;&gt;Real-Time Considerations&lt;/h2&gt;

&lt;p&gt;In most fields, real-time performance is very difficult, as each part of the pipeline must be optimized in order to minimize latency. In the case of source separation, audio collection from the input microphone must be parallelized with the neural network that is processing the data. In addition, the data must be divided into chunks which are passed through the neural network. However, if the neural network cannot process the input audio faster than it is coming in, the latency will still accumulate. In essence, this becomes a &lt;a href=&quot;https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem&quot;&gt;producer-consumer problem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some options for decreasing latency include: multithreading the python code, so audio can be recorded while the neural network performs computations, truncating the floating point precision, and using a faster language, such as C++. We are currently in the process of experimenting with these optimizations.&lt;/p&gt;

&lt;h2 id=&quot;results-and-next-steps&quot;&gt;Results and Next Steps&lt;/h2&gt;

&lt;p&gt;As of now, we have implemented a multithreaded python program which is able to chunk the data and process it through the neural network; however, the program has a reconstruction error that results in choppy sounding audio. The effects of this reconstruction error can be mitigated by increasing the chunk size, but that in turn increases latency.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To Be populated in with audio files&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On the one hand, we plan on investigating traditional reconstruction techniques to see if we can mitigate the choppiness. On the other, we also plan on training a neural network on data of the same length as our chosen chunk size, so that the training data is representative of our testing conditions.&lt;/p&gt;

&lt;p&gt;In order to decrease latency, we are looking into the &lt;a href=&quot;https://github.com/onnx/onnx&quot;&gt;ONNX standard&lt;/a&gt; and &lt;a href=&quot;https://developer.nvidia.com/tensorrt&quot;&gt;TensorRT&lt;/a&gt;, which should better optimize the neural network for fast compuation.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;https://www.ee.columbia.edu/~dpwe/papers/Cherry53-cpe.pdf &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1809.07454.pdf &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Richard Lee and Matt Kribs</name><email>Lee66@cooper.edu</email></author><summary type="html">Reaching Delayed Source Separation</summary></entry></feed>